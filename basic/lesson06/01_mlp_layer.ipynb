{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1 张量实现\n",
    "在 TensorFlow 中，要实现全连接层，只需要定义好权值张量𝑾和偏置张量𝒃，并利用\n",
    "TensorFlow 提供的批量矩阵相乘函数 tf.matmul()即可完成网络层的计算。例如，创建输入𝑿\n",
    "矩阵为𝑏 = 2个样本，每个样本的输入特征长度为𝑑in = 784，输出节点数为𝑑out = 256，故\n",
    "定义权值矩阵𝑾的 shape 为[784,256]，并采用正态分布初始化𝑾；偏置向量𝒃的 shape 定义\n",
    "为[256]，在计算完𝑿@𝑾后相加即可，最终全连接层的输出𝑶的 shape 为[2,256]，即 2 个样\n",
    "本的特征，每个特征长度为 256，代码实现如下："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # 创建 W,b 张量\n",
    "x = tf.random.normal([2,784])\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "o1 = tf.matmul(x,w1) + b1 # 线性变换\n",
    "o1 = tf.nn.relu(o1) # 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 层实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = tf.random.normal([4,28*28])\n",
    "from tensorflow.keras import layers # 导入层模块\n",
    "# 创建全连接层，指定输出节点数和激活函数\n",
    "fc = layers.Dense(512, activation=tf.nn.relu)\n",
    "h1 = fc(x) # 通过 fc 类实例完成一次全连接层的计算，返回输出张量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "上述通过一行代码即可以创建一层全连接层 fc，并指定输出节点数为 512，输入的节点数\n",
    "在fc(x)计算时自动获取，并创建内部权值张量𝑾和偏置张量𝒃。我们可以通过类内部的成\n",
    "员名 kernel 和 bias 来获取权值张量𝑾和偏置张量𝒃对象："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fc.kernel # 获取 Dense 类的权值矩阵"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fc.bias # 获取 Dense 类的偏置向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在优化参数时，需要获得网络的所有待优化的张量参数列表，可以通过类的\n",
    "trainable_variables 来返回待优化参数列表，代码如下："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fc.trainable_variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "实际上，网络层除了保存了待优化张量列表 trainable_variables，还有部分层包含了不\n",
    "参与梯度优化的张量，如后续介绍的 Batch Normalization 层，可以通过\n",
    "non_trainable_variables 成员返回所有不需要优化的参数列表。如果希望获得所有参数列\n",
    "表，可以通过类的 variables 返回所有内部张量列表，例如："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fc.variables # 返回所有参数列表"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "利用网络层类对象进行前向计算时，只需要调用类的__call__方法即可，即写成 fc(x)\n",
    "方式便可，它会自动调用类的__call__方法，在__call__方法中会自动调用 call 方法，这一\n",
    "设定由 TensorFlow 框架自动完成，因此用户只需要将网络层的前向计算逻辑实现在 call 方\n",
    "法中即可。对于全连接层类，在 call 方法中实现𝜎(𝑿@𝑾 + 𝒃)的运算逻辑，非常简单，最\n",
    "后返回全连接层的输出张量即可。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 神经网络"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
