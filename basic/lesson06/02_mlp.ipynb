{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 张量实现方式"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = tf.random.normal([2,784])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 隐藏层 1 张量\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 隐藏层 2 张量\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 隐藏层 3 张量\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 64], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([64]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 输出层张量\n",
    "w4 = tf.Variable(tf.random.truncated_normal([64, 10], stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在计算时，只需要按照网络层的顺序，将上一层的输出作为当前层的输入即可，重复\n",
    "直至最后一层，并将输出层的输出作为网络的输出，代码如下："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape: # 梯度记录器\n",
    "     # x: [b, 28*28]\n",
    "     # 隐藏层 1 前向计算，[b, 28*28] => [b, 256]\n",
    "     h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
    "     h1 = tf.nn.relu(h1)\n",
    "     # 隐藏层 2 前向计算，[b, 256] => [b, 128]\n",
    "     h2 = h1@w2 + b2\n",
    "     h2 = tf.nn.relu(h2)\n",
    "     # 隐藏层 3 前向计算，[b, 128] => [b, 64]\n",
    "     h3 = h2@w3 + b3\n",
    "     h3 = tf.nn.relu(h3)\n",
    "     # 输出层前向计算，[b, 64] => [b, 10]\n",
    "     h4 = h3@w4 + b4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在使用 TensorFlow 自动求导功能计算梯度时，需要将前向计算过程放置在\n",
    "tf.GradientTape()环境中，从而利用 GradientTape 对象的 gradient()方法自动求解参数的梯\n",
    "度，并利用 optimizers 对象更新参数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 层实现方式"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 导入常用网络层 layers\n",
    "from tensorflow.keras import layers,Sequential\n",
    "fc1 = layers.Dense(256, activation=tf.nn.relu) # 隐藏层 1\n",
    "fc2 = layers.Dense(128, activation=tf.nn.relu) # 隐藏层 2\n",
    "fc3 = layers.Dense(64, activation=tf.nn.relu) # 隐藏层 3\n",
    "fc4 = layers.Dense(10, activation=None) # 输出层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = tf.random.normal([4,28*28])\n",
    "h1 = fc1(x) # 通过隐藏层 1 得到输出\n",
    "h2 = fc2(h1) # 通过隐藏层 2 得到输出\n",
    "h3 = fc3(h2) # 通过隐藏层 3 得到输出\n",
    "h4 = fc4(h3) # 通过输出层得到网络输出\n",
    "h4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# sequential 实现\n",
    "对于这种数据依次向前传播的网络，也可以通过 Sequential 容器封装成一个网络大类\n",
    "对象，调用大类的前向计算函数一次即可完成所有层的前向计算，使用起来更加方便"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 导入 Sequential 容器\n",
    "from tensorflow.keras import layers,Sequential\n",
    "# 通过 Sequential 容器封装为一个网络类\n",
    "model = Sequential([\n",
    "  layers.Dense(256, activation=tf.nn.relu) , # 创建隐藏层 1\n",
    "  layers.Dense(128, activation=tf.nn.relu) , # 创建隐藏层 2\n",
    "  layers.Dense(64, activation=tf.nn.relu) , # 创建隐藏层 3\n",
    "  layers.Dense(10, activation=None) , # 创建输出层\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = model(x) # 前向计算得到输出"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
