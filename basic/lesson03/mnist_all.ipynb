{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 导入数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf # 导入 TF 库\n",
    "from tensorflow import keras # 导入 TF 子库 keras\n",
    "from tensorflow.keras import layers, optimizers, datasets # 导入 TF 子库等"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data() # 加载 MNIST 数据集\n",
    "x = 2 * tf.convert_to_tensor(x, dtype=tf.float32)/255.-1 # 转换为浮点张量，并缩放到-1~1\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32) # 转换为整形张量\n",
    "# y = tf.one_hot(y, depth=10) # one-hot 编码\n",
    "print(x.shape, y.shape)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y)) # 构建数据集对象\n",
    "train_dataset = train_dataset.batch(512) # 批量训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 网络搭建\n",
    "对于第一层模型来说，它接受的输入𝒙 ∈ 𝑅784，输出𝒉1 ∈ 𝑅256设计为长度为 256 的向\n",
    "量，我们不需要显式地编写𝒉1 = ReLU(𝑾1𝒙 + 𝒃1)的计算逻辑，在 TensorFlow 中通过一行代码即可实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.layers.core.Dense at 0x18164103100>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "# 创建一层网络，设置输出节点数为 256，激活函数类型为 ReLU\n",
    "layers.Dense(256, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用 TensorFlow 的 Sequential 容器可以非常方便地搭建多层的网络。对于 3 层网络，我们\n",
    "可以通过快速完成 3 层网络的搭建。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# 利用 Sequential 容器封装 3 个网络层，前网络层的输出默认作为下一层的输入\n",
    "model = keras.Sequential([ # 3 个非线性层的嵌套模型\n",
    " layers.Dense(256, activation='relu'), # 隐藏层 1\n",
    " layers.Dense(128, activation='relu'), # 隐藏层 2\n",
    " layers.Dense(10)]) # 输出层，输出节点数为 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "第 1 层的输出节点数设计为 256，第 2 层设计为 128，输出层节点数设计为 10。直接调用\n",
    "这个模型对象 model(x)就可以返回模型最后一层的输出𝑜。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.模型训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "搭建完成 3 层神经网络的对象后，给定输入𝒙，调用 model(𝒙)得到模型输出𝑜后，通过MSE 损失函数计算当前的误差ℒ："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD(learning_rate=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=2>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape: # 构建梯度记录环境\n",
    "    # 打平操作，[b, 28, 28] => [b, 784]\n",
    "    x = tf.reshape(x, (-1, 28*28))\n",
    "    # Step1. 得到模型输出 output [b, 784] => [b, 10]\n",
    "    out = model(x)\n",
    "    # [b] => [b, 10]\n",
    "    y_onehot = tf.one_hot(y, depth=10)\n",
    "    # 计算差的平方和，[b, 10]\n",
    "    loss = tf.square(out-y_onehot)\n",
    "    # 计算每个样本的平均误差，[b]\n",
    "    loss = tf.reduce_sum(loss) / x.shape[0]\n",
    "# Step3. 计算参数的梯度 w1, w2, w3, b1, b2, b3\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "# w' = w - lr * grad，更新网络参数\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "再利用 TensorFlow 提供的自动求导函数 tape.gradient(loss, model.trainable_variables)求出模\n",
    "型中所有参数的梯度信息𝜕ℒ/𝜕𝜃 , 𝜃 ∈ {𝑾1, 𝒃1,𝑾2, 𝒃2,𝑾3, 𝒃3}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
